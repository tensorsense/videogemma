{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\"custom_llava\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"custom_llava/checkpoints/llava_gemma_v1_pretrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_llava.llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    DEFAULT_IMAGE_PATCH_TOKEN,\n",
    ")\n",
    "from custom_llava.llava.conversation import conv_templates, SeparatorStyle\n",
    "from custom_llava.llava.model.builder import load_pretrained_model\n",
    "from custom_llava.llava.utils import disable_torch_init\n",
    "from custom_llava.llava.mm_utils import (\n",
    "    tokenizer_image_token,\n",
    "    process_images,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "from custom_llava.llava.model import (\n",
    "    LlavaConfig,\n",
    "    LlavaMistralForCausalLM,\n",
    "    LlavaLlamaForCausalLM,\n",
    "    LlavaGemmaForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LlavaGemmaConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m get_model_name_from_path(model_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_name = \"llava_gemma\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer, model, image_processor, context_len \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/vlm_sandbox/custom_llava/llava/model/builder.py:119\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# ==== UPDATED FOR LLM BACKEND ====\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     model \u001b[38;5;241m=\u001b[39m LlavaGemmaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    121\u001b[0m         model_path,\n\u001b[1;32m    122\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# =================================\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm_sandbox/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m model_type \u001b[38;5;241m=\u001b[39m config_class_to_model_type(\u001b[38;5;28mtype\u001b[39m(config)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m \u001b[43mTOKENIZER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm_sandbox/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:731\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[0;32m--> 731\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reverse_config_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    733\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LlavaGemmaConfig'"
     ]
    }
   ],
   "source": [
    "disable_torch_init()\n",
    "model_path = os.path.expanduser(model_path)\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "# model_name = \"llava_gemma\"\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda\",\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageBindVideoTower(\n",
       "  (video_tower): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (patch_dropout): PatchDropout()\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (temporal_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (temporal_layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vision_tower().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mode = \"gemma\"\n",
    "num_chunks = 1\n",
    "chunk_idx = 0\n",
    "temperature = 0.5\n",
    "top_p = None\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (1): GELU(approximate='none')\n",
       "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = \"What is unusual about this image?\"\n",
    "cur_prompt = qs\n",
    "\n",
    "# Insert special image tokens into the text prompt\n",
    "\n",
    "if model.config.mm_use_im_start_end:\n",
    "    image_tokens = \" \".join([DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN] * model.get_vision_tower().config.num_frames) + \"\\n\"\n",
    "else:\n",
    "    image_tokens = \" \".join([DEFAULT_IMAGE_TOKEN] * model.get_vision_tower().config.num_frames) + \"\\n\"\n",
    "qs = image_tokens + qs\n",
    "\n",
    "# Construct conversation prompt\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "# url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image_tensor = process_images([image], image_processor, model.config)[0]\n",
    "\n",
    "video = '1.mp4'\n",
    "video_tensor = image_processor(video, return_tensors='pt')['pixel_values'].to(\"cuda\")\n",
    "model.get_model().mm_projector.to(\"cuda\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs = \"What is unusual about this image?\"\n",
    "# cur_prompt = qs\n",
    "\n",
    "# conv = conv_templates[conv_mode].copy()\n",
    "# conv.append_message(conv.roles[0], qs)\n",
    "# conv.append_message(conv.roles[1], None)\n",
    "# prompt = conv.get_prompt()\n",
    "\n",
    "# input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = model.get_model().embed_tokens(input_ids)\n",
    "# print(input_ids.shape)\n",
    "# embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The person is sitting in close proximity to the camera. There is no indication of discomfort or evasion, and the individual appears to be enjoying the process. The lie detector confirms the truthfulness of the answers, indicating a genuine reflection on the subject matter.\\nIn this context, the individual's demeanor is light-hearted and candid, with moments of self-deprecation and humor. The person's body language is open, and they speak with a sense of ease, showing no signs of nervousness or deception.\\nThe individual's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person's body language is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation. The lie detector's confirmation of truthfulness suggests a genuine reflection on the subject matter.\\nThe individual's demeanor is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation. The lie detector's confirmation of truthfulness suggests a genuine reflection on the subject matter.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted.\\nThe individual's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person's body language is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation. The lie detector's confirmation of truthfulness suggests a genuine reflection on the subject matter.\\nThe person's demeanor is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation. The lie detector's confirmation of truthfulness suggests a genuine reflection on the subject matter.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted.\\nThe individual's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person's body language is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation. The lie detector's confirmation of truthfulness suggests a genuine reflection on the subject matter.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted.\\nThe person's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted.\\nThe person's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted.\\nThe person's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted. The person's body language is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation.\\nThe person's responses to questions about their personal life and experiences are delivered with a mix of humor and sincerity. The lie detector does not signal any deception, and the person's emotional state is one of reflection and openness.\\nThe person is sitting in close proximity to the camera, and the lie detector does not indicate any deception. The person is speaking with a sense of ease, and the mood is light-hearted. The person's body language is relaxed, and they speak with a sense of ease, indicating a level of comfort with the conversation.\\nThe person's responses to questions about their\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video_tensor.half().cuda(),\n",
    "        # image_sizes=[image.size],\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True)\n",
    "\n",
    "# print(output_ids)\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)\n",
    "tokenizer.apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
